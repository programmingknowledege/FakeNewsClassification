{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6052104a-ccb0-4d9d-9b71-ef877065e54b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, explode, from_json, lit, current_timestamp\n",
    "\n",
    "spark = SparkSession.builder.appName('kafka-stream') \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c70c3afa-2d7b-4b13-bd74-b19648633ccc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Kafka Schema including source\n",
    "json_schema = \"array<struct<title:string, text:string, label:int, id:string, date:string>>\"\n",
    "\n",
    "#Read from Kafka\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ec2-13-60-88-247.eu-north-1.compute.amazonaws.com:9092\")\n",
    "    .option(\"subscribe\", \"news_topic\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"failOnDataLoss\", \"false\") \n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json_string\")\n",
    ")\n",
    "\n",
    "#Parse the JSON data\n",
    "parsed_df = kafka_df.select(\n",
    "    explode(\n",
    "        from_json(\"json_string\", json_schema)\n",
    "    ).alias(\"data\")\n",
    ")\n",
    "\n",
    "final_df = parsed_df.select(\n",
    "    col(\"data.title\"),\n",
    "    col(\"data.text\"),\n",
    "    col(\"data.label\"),\n",
    "    col(\"data.id\"),\n",
    "    lit(\"kafka\").alias(\"source\"),  #Include source as 'kafka'\n",
    ")\n",
    "\n",
    "#Check point file had to be stored in a S3 bucket due to some access issues on ADLSgen2\n",
    "checkpoint_location = \"/mnt/2024-team2-s3a/stream\"\n",
    "\n",
    "query = final_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"path\", \"/mnt/delta/news_data\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "kafka",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
