{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b38b26e7-6944-496b-9e44-3433e22e9de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import lit, col, trim, when, current_timestamp, sha1\n",
    "\n",
    "\n",
    "#Define the schema for true and false data column\n",
    "common_schema = StructType([\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('date', StringType(), True)\n",
    "])\n",
    "\n",
    "schema1 = StructType([\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('label', IntegerType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('author', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('label', IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d3ce569-3ebf-4e52-9b70-e735afbd1d14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Merge the two files\n",
    "df1 = spark.read.schema(common_schema).csv(\"/mnt/2024-team2/dataset/data_true.csv\").withColumn('label', lit(1))\n",
    "df2 = spark.read.schema(common_schema).csv(\"/mnt/2024-team2/dataset/data_false.csv\").withColumn('label', lit(0))\n",
    "df1 = df1.drop('subject')\n",
    "df2 = df2.drop('subject')\n",
    "\n",
    "#Reading and modifying df3\n",
    "df3 = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").schema(schema1).csv(\"/mnt/2024-team2/dataset/dataset3.csv\")\n",
    "df3 = df3.withColumn(\"title\", lit(None)).withColumn(\"date\", lit(None))\n",
    "df3 = df3.withColumn(\"label\", when(col(\"label\") == 1, 0).otherwise(1))\n",
    "\n",
    "#Reading and modifying df4\n",
    "df4 = spark.read.option(\"header\", \"true\")\\\n",
    "                .option(\"sep\", \",\")\\\n",
    "                .option(\"quote\", \"\\\"\")\\\n",
    "                .option(\"escape\", \"\\\"\")\\\n",
    "                .option(\"multiLine\", \"true\")\\\n",
    "                .csv(\"/mnt/2024-team2/dataset/dataset4.csv\").select('title', 'text', 'label')\\\n",
    "                .withColumn(\"date\", lit(None))\n",
    "df4 = df4.withColumn(\"label\", when(col(\"label\") == 1, 0).otherwise(1))\n",
    "\n",
    "#Combine all DataFrames into one unified DataFrame\n",
    "df = df1.unionByName(df2).unionByName(df3).unionByName(df4)\n",
    "\n",
    "#Show the combined DataFrame with all expected columns\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3cb9a09-143b-4a45-886a-e9b377dd58a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.option(\"header\", \"true\").csv(\"/mnt/2024-team2/dataset/data_true.csv\").withColumn(\"label\", lit(1))\n",
    "df2 = spark.read.option(\"header\", \"true\").csv(\"/mnt/2024-team2/dataset/data_false.csv\").withColumn(\"label\", lit(0))\n",
    "\n",
    "#Drop unnecessary columns if present\n",
    "if \"subject\" in df1.columns:\n",
    "    df1 = df1.drop(\"subject\")\n",
    "if \"subject\" in df2.columns:\n",
    "    df2 = df2.drop(\"subject\")\n",
    "\n",
    "#Reading CSV files for df3 and df4\n",
    "df3 = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").csv(\"/mnt/2024-team2/dataset/dataset3.csv\")\n",
    "df4 = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").option(\"multiLine\", \"true\").csv(\"/mnt/2024-team2/dataset/dataset4.csv\")\n",
    "\n",
    "#Adjust columns to match (add missing columns with default values)\n",
    "df3 = df3.withColumn(\"date\", lit(None)).withColumn(\"title\", lit(None))\n",
    "df4 = df4.withColumn(\"date\", lit(None)).withColumn(\"title\", lit(None))\n",
    "\n",
    "#Optionally invert labels in df3 and df4 if required\n",
    "df3 = df3.withColumn(\"label\", when(col(\"label\") == 1, 0).otherwise(1))\n",
    "df4 = df4.withColumn(\"label\", when(col(\"label\") == 1, 0).otherwise(1))\n",
    "\n",
    "#Select only necessary columns to ensure schema alignment\n",
    "df1 = df1.select(\"title\", \"text\", \"date\", \"label\")\n",
    "df2 = df2.select(\"title\", \"text\", \"date\", \"label\")\n",
    "df3 = df3.select(\"title\", \"text\", \"date\", \"label\")\n",
    "df4 = df4.select(\"title\", \"text\", \"date\", \"label\")\n",
    "\n",
    "#Union all DataFrames\n",
    "df_unioned = df1.unionByName(df2).unionByName(df3).unionByName(df4)\n",
    "\n",
    "#Show the results\n",
    "df_unioned.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8eb4d3c-5c64-4e76-9cbc-2bb7cb595456",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Uniform the third file\n",
    "df5 = spark.read.csv(\"/mnt/2024-team2/dataset/welfake_data.csv\",header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df5 = df5.drop('_c0')\n",
    "\n",
    "df5 = df5.withColumn('date', lit(None).cast(StringType()))\n",
    "df5 = df5.withColumn('label', col('label').cast(IntegerType()))\n",
    "\n",
    "df_combined = df_unioned.unionByName(df5)\n",
    "df_combined.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69fcfde6-c025-4cac-8465-586d57fcf89a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_combined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b1d96e-a4ba-4d1d-94d1-7388b5709dbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_combined.filter(df_combined.text.isNotNull() & df_combined.label.isNotNull())\n",
    "df_cleaned = df_cleaned.filter(col('label').isin([0, 1]))\n",
    "df_cleaned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e634ccc-ddce-43aa-9fe5-bbf69b09a003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned_filtered = df_cleaned.filter(trim(df_cleaned.text) != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "653cb7f9-c3c5-468c-aecd-b7d5460192a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned_filtered.schema\n",
    "df = df_cleaned_filtered.withColumn(\"timestamp\", current_timestamp())\n",
    "df = df.withColumn(\"id\", sha1(\"text\"))\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27265cfd-df86-4734-a1ae-e231b1df8b0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ba5d6d9-82a8-408a-8fa9-9ac22a074228",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create delta table\n",
    "delta_table_path = \"/mnt/delta/news_delta\"\n",
    "\n",
    "# Write the DataFrame to the Delta format\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515c9d37-534f-4a62-b88d-433b23e88f42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE news_delta USING DELTA LOCATION '/mnt/delta/news_delta'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264d0573-0eaf-4e9f-8952-ce95df5b39fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM news_data;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "data-ingestion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
