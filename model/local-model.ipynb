{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00b9e098-2935-4d04-a775-ffa37fac6c36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, HashingTF, IDF, StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import lit, regexp_replace, lower, explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37745a0d-f5b2-456c-abc1-0afd3120e6c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fake_news_dataset = spark.read.format(\"delta\").table(\"news_data\")\n",
    "common_schema = StructType([\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('date', StringType(), True)\n",
    "])\n",
    "\n",
    "fake_news_dataset.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51d7f604-7333-427b-ad61-4b629c1c6c67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Selecting only the 'label' and 'text' columns from the fake_news_dataset DataFrame\n",
    "fake_news_dataset = fake_news_dataset.select('label', 'text')\n",
    "\n",
    "#Filling any missing values\n",
    "fake_news_dataset = fake_news_dataset.fillna({'text': ''})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bec5313-4647-4d6f-bebe-87b61467fa5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset into training and test datasets with a ratio of 70% for training and 30% for testing\n",
    "#he seed parameter ensures reproducibility of the random split\n",
    "(training_fake_news_data, test_fake_news_data) = fake_news_dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "#Getting the column names of the fake_news_dataset DataFrame\n",
    "columnnames = fake_news_dataset.columns\n",
    "\n",
    "#Printing the column names\n",
    "columnnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0610d67-d5ea-486c-b9e5-154e818a1b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "punctuation_chars = '!#$%&'\n",
    "\n",
    "def preprocessing_text_data(dataset, punctuation_chars):\n",
    "    #Convert text to lowercase\n",
    "    dataset = dataset.withColumn('text', lower(dataset['text']))\n",
    "\n",
    "    #Remove URLs\n",
    "    dataset = dataset.withColumn(\"text\", regexp_replace(dataset[\"text\"], r\"http[s]?\\://\\S+\", \"\"))\n",
    "\n",
    "    #Remove text within parentheses or square brackets\n",
    "    dataset = dataset.withColumn(\"text\", regexp_replace(dataset[\"text\"], r\"(\\(.*\\))|(\\[.*\\])\", \"\"))\n",
    "\n",
    "    #Remove words containing consecutive asterisks\n",
    "    dataset = dataset.withColumn(\"text\", regexp_replace(dataset[\"text\"], r\"\\b\\w+\\*{2,3}\\w*\\b\", \"\"))\n",
    "\n",
    "    #Remove special characters and punctuation\n",
    "    dataset = dataset.withColumn(\"text\", regexp_replace(dataset[\"text\"], r'[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}]+', \"\"))\n",
    "    dataset = dataset.withColumn(\"text\", regexp_replace(dataset[\"text\"], r\"[\" + re.escape(punctuation_chars) + \"]\", \"\"))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "training_fake_news_data = preprocessing_text_data(training_fake_news_data,punctuation_chars)\n",
    "training_fake_news_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56437eb7-683d-45a7-8000-b412bbcd3f0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "training_fake_news_data = regex_tokenizer.transform(training_fake_news_data)\n",
    "training_fake_news_data.select('label', 'text', 'words').show(5)\n",
    "# tokenizer.save(\"/mnt/2024-team2/local/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc5ec3b-aabd-401e-b861-49a875e7f207",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "training_fake_news_data = stopwords_remover.transform(training_fake_news_data)\n",
    "training_fake_news_data.show(10)\n",
    "# stopwords_remover.save(\"/mnt/2024-team2/local/stopwords_remover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2669c1ee-d3d7-4382-bccd-fc6130ba7ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a HashingTF (Term Frequency) instance with input and output columns specified, and the number of features set to 1000\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\", outputCol=\"raw_features\", numFeatures=3000)\n",
    "\n",
    "#Transform the training_fake_news_data using the HashingTF model to obtain the raw feature vectors\n",
    "featurized_data = hashing_tf.transform(training_fake_news_data)\n",
    "\n",
    "# hashing_tf.save(\"/mnt/2024-team2/local/hashing_tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "991138d2-ff20-4853-bfec-f827c48aef81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "featurized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc7f0aa7-ce09-4047-ba86-ae424bcc8dae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create an IDF instance \n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "#Fit the IDF model to the featurized data to compute the IDF weights\n",
    "idf_vectorizer = idf.fit(featurized_data)\n",
    "\n",
    "#Transform the featurized data using the IDF model to obtain the TF-IDF weighted vectors\n",
    "rescaled_data = idf_vectorizer.transform(featurized_data)\n",
    "\n",
    "#Display the transformed data with TF-IDF weighted vectors\n",
    "rescaled_data.show()\n",
    "\n",
    "# idf_vectorizer.save(\"/mnt/2024-team2/local/idf_vectorizer\")\n",
    "# rescaled_data.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6d04aff-7dcf-4087-a683-0bbf5348f0f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rescaled_data.show()\n",
    "type(rescaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c58092c-03ef-483d-a009-ec87ba3e9346",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Global variable\n",
    "global column_names\n",
    "column_names = rescaled_data.columns\n",
    "\n",
    "#Build a logistic regression model on each partition of data\n",
    "def build_model(partition_data_it):\n",
    "    try:\n",
    "        print('Inside build_model')\n",
    "        \n",
    "        # Convert the partition data iterator to a pandas DataFrame with column names\n",
    "        partition_data_it = pd.DataFrame(partition_data_it, columns=column_names)\n",
    "        \n",
    "        # Extract features and labels from the partition data\n",
    "        X_train = list(partition_data_it['features'])\n",
    "        Y_train = partition_data_it['label']\n",
    "        \n",
    "        # Initialize and train a logistic regression model\n",
    "        clf = LogisticRegression()\n",
    "        model = clf.fit(X_train, Y_train)\n",
    "        \n",
    "        # Return the trained model\n",
    "        return [model]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Inside Exception')\n",
    "        import traceback\n",
    "        print(traceback.print_exc)\n",
    "        print(e)\n",
    "\n",
    "#Repartition the rescaled data RDD into 5 partitions\n",
    "training_fake_news_rdd = rescaled_data.rdd.repartition(5)\n",
    "type(training_fake_news_rdd)\n",
    "\n",
    "#Apply the build_model function to each partition of the RDD\n",
    "transformed_fake_news_rdd = training_fake_news_rdd.mapPartitions(build_model)\n",
    "\n",
    "# collect the models generated from each partition\n",
    "try:\n",
    "    models = transformed_fake_news_rdd.collect()\n",
    "    print(\"Transformation successful.\")\n",
    "except Exception as e:\n",
    "    print(\"Error during transformation:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac1f15a9-9162-42e1-83ad-ff0b42ceedec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0efe6462-bff1-4602-9e7e-6d50ca5b649f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define a function to predict the label for a given instance using a list of models\n",
    "def predict(instance):\n",
    "    # Predict the label for the instance using each model in the list of models\n",
    "    # Return a list of predictions\n",
    "    return [m.predict([instance['features']]) for m in models]\n",
    "\n",
    "#Define a function to aggregate predictions and determine the final label\n",
    "def agg_predictions(preds):\n",
    "    #Initialize a dictionary to store the count of each label\n",
    "    prediction = {0: 0, 1: 0}\n",
    "    \n",
    "    #Iterate over the predictions and update the count for each label\n",
    "    for elem in preds:\n",
    "        prediction[elem[0]] += 1\n",
    "    \n",
    "    #Return the label with the highest count\n",
    "    return max(prediction, key=prediction.get)\n",
    "\n",
    "#Preprocess the test data\n",
    "test_fake_news_data = preprocessing_text_data(test_fake_news_data, punctuation_chars)\n",
    "test_fake_news_data = regex_tokenizer.transform(test_fake_news_data)\n",
    "test_fake_news_data = stopwords_remover.transform(test_fake_news_data)\n",
    "featurized_data = hashing_tf.transform(test_fake_news_data)\n",
    "test_fake_news_data = idf_vectorizer.transform(featurized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bdbd186-f38f-4efe-b3ec-019f25b40f38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "\n",
    "#Define a function to transform each instance in the test data\n",
    "def transform(instance):\n",
    "    #Generate raw predictions for the instance using the agg_predictions function\n",
    "    #Convert the result to a Row object\n",
    "    return Row(**instance.asDict(), raw_prediction=agg_predictions(predict(instance)))\n",
    "\n",
    "#Repartition the testData RDD into 10 partitions and apply the transform function to each instance\n",
    "#Convert the transformed RDD to a DataFrame named 'prediction'\n",
    "prediction = test_fake_news_data.rdd.repartition(10).map(transform).toDF()\n",
    "\n",
    "#Show the contents of the 'prediction' DataFrame\n",
    "prediction.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "634e394b-e065-4a23-835e-dc298f53d958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#This code is return to test the model on different partition of the dataset. \n",
    "partition_accuracy={}\n",
    "for partition in range(5,20):\n",
    "  training_fake_news_rdd = rescaled_data.rdd.repartition(partition)\n",
    "  transformed_fake_news_rdd = training_fake_news_rdd.mapPartitions(build_model)\n",
    "  try:\n",
    "      models = transformed_fake_news_rdd.collect()\n",
    "      print(\"Transformation successful.\")\n",
    "  except Exception as e:\n",
    "    print(\"Error during transformation:\", e) \n",
    "  prediction=test_fake_news_data.rdd.repartition(partition).map(transform).toDF()\n",
    "  prediction_num=prediction.select((prediction['label']==0).cast('double').alias('label'),\n",
    "                                  (prediction['raw_prediction']==0).cast('double').alias('raw_prediction'),\n",
    "                                  )\n",
    "  \n",
    "  acc_evaluator=MulticlassClassificationEvaluator(metricName='accuracy',labelCol='label',predictionCol='raw_prediction')\n",
    "  accuracy=acc_evaluator.evaluate(prediction_num)\n",
    "  partition_accuracy[partition]=accuracy\n",
    "print(partition_accuracy)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce55d01f-3fcb-4e75-a3f6-fdad62e5ac93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction.cache()\n",
    "prediction_num=prediction.select((prediction['label']==0).cast('double').alias('label'),\n",
    "                                 (prediction['raw_prediction']==0).cast('double').alias('raw_prediction'),\n",
    "                                 )\n",
    "\n",
    "acc_evaluator=MulticlassClassificationEvaluator(metricName='accuracy',labelCol='label',predictionCol='raw_prediction')\n",
    "prediction_num.cache()\n",
    "acc_evaluator.evaluate(prediction_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fd12e07-e5b6-4808-ab93-63afb4d0ccc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Extracting partition values and corresponding accuracy values from the dictionary\n",
    "parition_value = list(partition_accuracy.keys())\n",
    "partition_accuracy = list(partition_accuracy.values())\n",
    "\n",
    "#Plotting the accuracy vs number of partitions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(parition_value, partition_accuracy, marker='o', linestyle='-')\n",
    "plt.title('Accuracy vs Number of Partitions')\n",
    "plt.xlabel('Number of Partitions')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.xticks(parition_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa0bac82-56a7-4670-9c35-73b0b834118c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41f4b839-4566-48f6-bee4-a64fe9ef5cef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculate various evaluation metrics \n",
    "accuracy = acc_evaluator.evaluate(prediction_num, {acc_evaluator.metricName: \"accuracy\"})\n",
    "precision = acc_evaluator.evaluate(prediction_num, {acc_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = acc_evaluator.evaluate(prediction_num, {acc_evaluator.metricName: \"weightedRecall\"})\n",
    "f1_score = acc_evaluator.evaluate(prediction_num, {acc_evaluator.metricName: \"f1\"})\n",
    "\n",
    "#Print the confusion metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc70e45-52e3-45a1-91ae-e83062de33e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "local-model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
