{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c63a24d-b059-425d-b41e-7247b39f519e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re \n",
    "#Import pyspark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import RegexTokenizer, Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator,Evaluator\n",
    "from pyspark.sql.functions import lit, regexp_replace, lower, explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml.tuning import CrossValidator, TrainValidationSplit\n",
    "from pyspark.ml.param.shared import HasSeed\n",
    "from pyspark.ml.util import _jvm\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pyspark.ml.feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2106606e-1382-4fac-8225-45c8cdc20f9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Read fake new data from the delta table\n",
    "fake_news_dataset = spark.read.format(\"delta\").table(\"news_data\")\n",
    "\n",
    "#Display top 5 row of dataframe\n",
    "schema = StructType([\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('date', StringType(), True)\n",
    "])\n",
    "fake_news_dataset=fake_news_dataset.select('text','label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac8908a-4f75-43e7-aa78-5bfe24360ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the 'fake_news_dataset' into training and test sets with a ratio of 70% to 30% respectively\n",
    "\n",
    "(training_fake_news_data, test_fake_news_data) = fake_news_dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "#Print the columns\n",
    "columnnames = training_fake_news_data.columns\n",
    "\n",
    "# Print or retrieve the column names\n",
    "columnnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f11ba74-ca58-4691-8b91-c30592079e7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(label=0, count=84623), Row(label=1, count=70146)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data in 'training_fake_news_data' by the 'label' column and count the occurrences of each label\n",
    "value_counts = training_fake_news_data.groupBy('label').count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Collect the count data from the Spark DataFrame to the driver as a list of rows\n",
    "value_counts.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245026b0-46c9-437e-8e09-e6b977d2404c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------+\n|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |label|clean_text                                       |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------+\n|    (Welcome) to America We hope you enjoy our benefits. Please feel free to NOT assimilate  Coming soon: Press 1 for English, 2 for Spanish and 3 for Arabic The Obama Administration is on pace to issue more than a million green cards to migrants from majority-Muslim countries, according to an analysis of Department of Homeland Security data.A chart released by the Senate Subcommittee on Immigration and the National Interest Friday details the surge in immigration to the U.S. from majority-Muslim countries since President Barack Obama took office in 2009.Specifically, in the first six fiscal years of Obama s presidency (FY2009   FY2014), his administration issued 832,014 green cards to migrants majority-Muslim countries, the most of which were issued to migrants from Pakistan (102,000), Iraq (102,000), Bangladesh (90,000), Iran (85,000), Egypt (56,000), and Somalia (37,000).The total 832,014 new permanent residents do not include migrants on temporary, nonimmigrant visas   which allow foreign nationals to come to the U.S. temporarily for work, study, tourism and the like. As the subcommittee notes, the number also does not include those migrants who overstayed the terms of their visas.Regardless, as the subcommittee explained in its analysis, the U.S. is playing host to immigrants from majority Muslim countries at an increasing pace.Between FY 2013 and FY 2014, the number of green cards issued to migrants from Muslim-majority countries increased dramatically   from 117,423 in FY 2013, to 148,810 in FY 2014, a nearly 27 percent increase. Throughout the Obama Administration s tenure, the United States has issued green cards to an average of 138,669 migrants from Muslim-majority countries per year, meaning that it is nearly certain the United States will have issued green cards to at least 1.1 million migrants from Muslim-majority countries on the President s watch. It has also been reported that migration from Muslim-majority countries represents the fastest growing class of migrants.Green cards, or Lawful Permanent Residency, puts immigrants on the path to citizenship and allows for lifetime residency, federal benefits, and work authorization. Included in the totals are refugees, who are required to apply for a green card after one year of residency in the U.S. Unlike other types of immigrants, refugees are immediately eligible for welfare benefits including Temporary Assistance to Needy Families (TANF), food stamps, and Medicaid. Via: Breitbart News |1    |     food stamps and medicaid via breitbart news |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "global punctuation_chars\n",
    "punctuation_chars = '!#$%&'\n",
    "\n",
    "def preprocessing_fake_news_text_data(dataset, punctuation_chars):\n",
    "    # Convert text to lowercase\n",
    "    dataset = dataset.withColumn('clean_text', lower(dataset['text']))\n",
    "    \n",
    "    # Remove URLs\n",
    "    dataset = dataset.withColumn(\"clean_text\", regexp_replace(dataset[\"clean_text\"], r\"http[s]?\\://\\S+\", \"\")) \n",
    "    \n",
    "    # Remove text within parentheses or square brackets\n",
    "    dataset = dataset.withColumn(\"clean_text\", regexp_replace(dataset[\"clean_text\"], r\"(\\(.*\\))|(\\[.*\\])\", \"\"))\n",
    "    \n",
    "    # Remove words containing consecutive asterisks\n",
    "    dataset = dataset.withColumn(\"clean_text\", regexp_replace(dataset[\"clean_text\"], r\"\\b\\w+\\*{2,3}\\w*\\b\", \"\"))\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    dataset = dataset.withColumn(\"clean_text\", regexp_replace(dataset[\"clean_text\"], r'[!#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}]+', \"\"))\n",
    "    dataset = dataset.withColumn(\"clean_text\", regexp_replace(dataset[\"clean_text\"], r\"[\" + re.escape(punctuation_chars) + \"]\", \"\"))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "training_fake_news_data = preprocessing_fake_news_text_data(training_fake_news_data,punctuation_chars)\n",
    "training_fake_news_data.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f6c2e1-2fba-40ef-8fc9-fe420645aa37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n|label|          clean_text|               words|\n+-----+--------------------+--------------------+\n|    1|     food stamps ...|[food, stamps, an...|\n|    1|     food stamps ...|[food, stamps, an...|\n|    0|    barbra streis...|[barbra, streisan...|\n|    0|    actor daniel ...|[actor, daniel, a...|\n|    0|    months after ...|[months, after, c...|\n+-----+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Regex Tokenizer breaks down text into individual words or tokens, useful for text processing tasks.\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Apply the tokenizer to the 'training_fake_news_data' DataFrame to create a new column 'words' \n",
    "training_fake_news_data = regex_tokenizer.transform(training_fake_news_data)\n",
    "\n",
    "# Select and display the first 5 rows of the DataFrame showing the 'label', 'clean_text', and 'words' columns.\n",
    "training_fake_news_data.select('label', 'clean_text', 'words').show(5)\n",
    "\n",
    "# regex_tokenizer.save(\"/mnt/2024-team2/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41506fa5-cfdd-43a1-a9fa-93bb5501832c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n|                text|label|          clean_text|               words|            filtered|\n+--------------------+-----+--------------------+--------------------+--------------------+\n|    (Welcome) to ...|    1|     food stamps ...|[food, stamps, an...|[food, stamps, me...|\n|    (Welcome) to ...|    1|     food stamps ...|[food, stamps, an...|[food, stamps, me...|\n|    Barbra Streis...|    0|    barbra streis...|[barbra, streisan...|[barbra, streisan...|\n|    actor Daniel ...|    0|    actor daniel ...|[actor, daniel, a...|[actor, daniel, a...|\n|    months after ...|    0|    months after ...|[months, after, c...|[months, comedy, ...|\n|    months after ...|    0|    months after ...|[months, after, c...|[months, comedy, ...|\n|   Afghanistan  —...|    0|   afghanistan  —...|[afghanistan, as,...|[afghanistan, tal...|\n|   Delta Air Line...|    1|   delta air line...|[delta, air, line...|[delta, air, line...|\n|   Delta Air Line...|    1|   delta air line...|[delta, air, line...|[delta, air, line...|\n|   France  —   Hi...|    0|   france  —   hi...|[france, his, bik...|[france, bike, cr...|\n+--------------------+-----+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Instantiate StopwordRemover object that will remove the stopwords from the words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "#Apply the tokenizer to the 'training_fake_news_data' DataFrame to create a new column 'words' \n",
    "training_fake_news_data = stopwords_remover.transform(training_fake_news_data)\n",
    "training_fake_news_data.show(10)\n",
    "\n",
    "# stopwords_remover.save((\"/mnt/2024-team2/stopwords_remover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34243bc1-88ea-46b7-ba12-611194ca9dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n|                text|label|          clean_text|               words|            filtered|        raw_features|\n+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n|    (Welcome) to ...|    1|     food stamps ...|[food, stamps, an...|[food, stamps, me...|(3000,[521,608,14...|\n|    (Welcome) to ...|    1|     food stamps ...|[food, stamps, an...|[food, stamps, me...|(3000,[521,608,14...|\n|    Barbra Streis...|    0|    barbra streis...|[barbra, streisan...|[barbra, streisan...|(3000,[7,15,85,10...|\n|    actor Daniel ...|    0|    actor daniel ...|[actor, daniel, a...|[actor, daniel, a...|(3000,[17,33,61,1...|\n|    months after ...|    0|    months after ...|[months, after, c...|[months, comedy, ...|(3000,[0,5,32,102...|\n|    months after ...|    0|    months after ...|[months, after, c...|[months, comedy, ...|(3000,[0,5,32,102...|\n|   Afghanistan  —...|    0|   afghanistan  —...|[afghanistan, as,...|[afghanistan, tal...|(3000,[22,41,55,6...|\n|   Delta Air Line...|    1|   delta air line...|[delta, air, line...|[delta, air, line...|(3000,[104,144,16...|\n|   Delta Air Line...|    1|   delta air line...|[delta, air, line...|[delta, air, line...|(3000,[104,144,16...|\n|   France  —   Hi...|    0|   france  —   hi...|[france, his, bik...|[france, bike, cr...|(3000,[20,29,30,4...|\n+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Define HashingTF transformer\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\", \n",
    "                       outputCol=\"raw_features\",  \n",
    "                       numFeatures=3000) \n",
    "\n",
    "#Transform the input data using HashingTF\n",
    "featurized_data = hashing_tf.transform(training_fake_news_data)\n",
    "\n",
    "#Show the first 10 rows of the transformed DataFrame\n",
    "featurized_data.show(10)\n",
    "\n",
    "# hashing_tf.save(\"/mnt/2024-team2/hashing_tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f8869d4-ee6c-4518-be28-06116ea07fcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|                text|label|          clean_text|               words|            filtered|        raw_features|            features|\n+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    (Welcome) to ...|    1|     food stamps ...|[food, stamps, an...|[food, stamps, me...|(3000,[521,608,14...|(3000,[521,608,14...|\n|    (Welcome) to ...|    1|     food stamps ...|[food, stamps, an...|[food, stamps, me...|(3000,[521,608,14...|(3000,[521,608,14...|\n|    Barbra Streis...|    0|    barbra streis...|[barbra, streisan...|[barbra, streisan...|(3000,[7,15,85,10...|(3000,[7,15,85,10...|\n|    actor Daniel ...|    0|    actor daniel ...|[actor, daniel, a...|[actor, daniel, a...|(3000,[17,33,61,1...|(3000,[17,33,61,1...|\n|    months after ...|    0|    months after ...|[months, after, c...|[months, comedy, ...|(3000,[0,5,32,102...|(3000,[0,5,32,102...|\n|    months after ...|    0|    months after ...|[months, after, c...|[months, comedy, ...|(3000,[0,5,32,102...|(3000,[0,5,32,102...|\n|   Afghanistan  —...|    0|   afghanistan  —...|[afghanistan, as,...|[afghanistan, tal...|(3000,[22,41,55,6...|(3000,[22,41,55,6...|\n|   Delta Air Line...|    1|   delta air line...|[delta, air, line...|[delta, air, line...|(3000,[104,144,16...|(3000,[104,144,16...|\n|   Delta Air Line...|    1|   delta air line...|[delta, air, line...|[delta, air, line...|(3000,[104,144,16...|(3000,[104,144,16...|\n|   France  —   Hi...|    0|   france  —   hi...|[france, his, bik...|[france, bike, cr...|(3000,[20,29,30,4...|(3000,[20,29,30,4...|\n+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Create IDF transformer object\n",
    "idf = IDF(inputCol=\"raw_features\",\n",
    "          outputCol=\"features\")\n",
    "\n",
    "#Fit IDF transformer to the featurized data\n",
    "idf_vectorizer = idf.fit(featurized_data)\n",
    "\n",
    "#Transform the featurized data using the trained IDF model\n",
    "rescaled_data = idf_vectorizer.transform(featurized_data)\n",
    "\n",
    "#displayed  the first 10 rows of the transformed DataFrame\n",
    "rescaled_data.show(10)\n",
    "\n",
    "# idf.save(\"/mnt/2024-team2/idf_model_path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf3e94a-0c8e-44da-93ee-f3497beb87f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomParamValidator:\n",
    "    '''Created Param Validation class that will validate the model on different params and return the model with best params and model'''\n",
    "\n",
    "    def __init__(self, estimator, customParamsList=None, evaluator=None, numFolds=3, seed=None):\n",
    "        self.estimator = estimator\n",
    "        self.customParamsList = customParamsList\n",
    "        self.evaluator = evaluator\n",
    "        \n",
    "    def _fit(self, dataset):\n",
    "        estimators = self.estimator\n",
    "        params = self.customParamsList\n",
    "        evaluator = self.evaluator\n",
    "        \n",
    "        #Custom cross-validation logic\n",
    "        bestModel = None\n",
    "        bestMetric = float('-inf')\n",
    "        bestParams = None\n",
    "        training_data, test_data = dataset.randomSplit([0.8, 0.2], seed=123)\n",
    "        for param in params:\n",
    "            model = estimators.fit(training_data, param)\n",
    "            metric = evaluator.evaluate(model.transform(test_data))\n",
    "            if metric > bestMetric:\n",
    "              bestMetric = metric\n",
    "              bestModel = model\n",
    "              bestParams = params\n",
    "\n",
    "        return bestModel,bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a1dc1f-ea4f-46f2-a4ef-2daf609c6a30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "estimator = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "customParamsList = [\n",
    "    {estimator.maxIter: 10, estimator.regParam: 0.1},\n",
    "    {estimator.maxIter: 20, estimator.regParam: 0.01},\n",
    "]\n",
    "\n",
    "#created binary class classsifier \n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "#Instantiate CustomParamValidator\n",
    "customCrossValidator = CustomParamValidator(estimator=estimator, customParamsList=customParamsList, evaluator=evaluator)\n",
    "\n",
    "bestModel, bestParams = customCrossValidator._fit(rescaled_data)\n",
    "# bestModel.save('/mnt/2024-team2/lr-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "218d0bc6-1d8f-4d00-8723-978790c89ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_cf81b02a4e99, numClasses=2, numFeatures=3000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#diplay best model\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8026076-9bdd-4493-8f32-f1ea13b9c4e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{Param(parent='LogisticRegression_cf81b02a4e99', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       "  Param(parent='LogisticRegression_cf81b02a4e99', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='LogisticRegression_cf81b02a4e99', name='maxIter', doc='max number of iterations (>= 0).'): 20,\n",
       "  Param(parent='LogisticRegression_cf81b02a4e99', name='regParam', doc='regularization parameter (>= 0).'): 0.01}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display bet params\n",
    "bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d197873f-27ae-4ff2-a0fd-47c9a168d429",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a MulticlassClassificationEvaluator object for evaluating model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe537b1-0993-45c9-b1ef-c65b768d83ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Preprocess the test data by cleaning the text and removing punctuation\n",
    "test_fake_news_data = preprocessing_fake_news_text_data(test_fake_news_data, punctuation_chars)\n",
    "\n",
    "#Tokenize the cleaned text using the regex tokenizer\n",
    "test_fake_news_data = regex_tokenizer.transform(test_fake_news_data)\n",
    "\n",
    "#Remove stop words from the tokenized text\n",
    "test_fake_news_data = stopwords_remover.transform(test_fake_news_data)\n",
    "\n",
    "#Transform the preprocessed test data into features\n",
    "featurized_test_data = hashing_tf.transform(test_fake_news_data)\n",
    "\n",
    "#Apply IDF transformation to the featurized test data\n",
    "rescaled_test_data = idf_vectorizer.transform(featurized_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6675c76d-6b53-4913-b142-5c701b8a9296",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Make predictions on the preprocessed and transformed test data using the best model obtained from custom param validation\n",
    "predictions = bestModel.transform(rescaled_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e8f3d5-997b-4c3a-883a-ad47e1b4f207",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Evaluate the performance of the model by computing the accuracy on the predictions made for the test data\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1dcb308-677b-4f09-b87b-1f6cbbae3ae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7263934574644657"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display accuracy of the model \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bfc6f89-f832-44af-8118-524f59685beb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7263934574644657\nPrecision: 0.7257502751136528\nRecall: 0.7263934574644657\nF1 Score: 0.7256117815834378\n"
     ]
    }
   ],
   "source": [
    "#calculate evaluaion metrics of the model \n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "#Print the confusion metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0382a60-5627-480c-8bd1-dea22b32eeab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# predict_saved = bestModel.transform(rescaled_data)\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')\n",
    "# accuracy = evaluator.evaluate(predict_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7781db53-7d1a-4cf4-b002-6cdc3eacf759",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\nTrue Positives: 20341\nFalse Positives: 8236\nTrue Negatives: 27800\nFalse Negatives: 9897\n[[20341, 9897], [8236, 27800]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#Compute confusion metrics\n",
    "confusion_metrics = predictions \\\n",
    "    .groupBy('prediction', 'label') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'count') \\\n",
    "    .orderBy('prediction', 'label')\n",
    "\n",
    "#Calculate true positives false positives true negatives and false negatives\n",
    "true_positives = confusion_metrics.filter(expr('prediction == 1 AND label == 1')).select(expr('sum(count)')).collect()[0][0]\n",
    "false_positives = confusion_metrics.filter(expr('prediction == 1 AND label == 0')).select(expr('sum(count)')).collect()[0][0]\n",
    "true_negatives = confusion_metrics.filter(expr('prediction == 0 AND label == 0')).select(expr('sum(count)')).collect()[0][0]\n",
    "false_negatives = confusion_metrics.filter(expr('prediction == 0 AND label == 1')).select(expr('sum(count)')).collect()[0][0]\n",
    "\n",
    "#Output the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"True Positives:\", true_positives)\n",
    "print(\"False Positives:\", false_positives)\n",
    "print(\"True Negatives:\", true_negatives)\n",
    "print(\"False Negatives:\", false_negatives)\n",
    "\n",
    "\n",
    "#Define confusion matrix values\n",
    "confusion_matrix = [[true_positives, false_negatives],\n",
    "                    [false_positives, true_negatives]]\n",
    "print(confusion_matrix)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1e1496-e07d-409e-b84f-33501158c282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "global-model",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
